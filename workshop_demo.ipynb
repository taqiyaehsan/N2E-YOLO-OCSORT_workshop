{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "import cv2 # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from ultralytics import YOLO # type: ignore\n",
    "from PIL import Image # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the YOLOv8 nano model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Object Detection on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('rutgers_yolo_demo.jpg')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(img)[0]\n",
    "results.show()\n",
    "\n",
    "print(f\" Number of objects detected: {len(results.boxes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze each box: \\\n",
    "    * xyxy – the coordinates of the box as an array [x1,y1,x2,y2] \\\n",
    "    * cls – the ID of object type\\\n",
    "    * conf – the confidence level of the model about this object. If it's very low, like < 0.5, then you can just ignore the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results.boxes)):\n",
    "    box = results.boxes[i]\n",
    "    print(f\"Box {i+1}\")\n",
    "    print(\"Object type:\", box.cls)\n",
    "    print(\"Coordinates:\", box.xyxy)\n",
    "    print(\"Probability:\", box.conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time Object Detection on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"yolo_demo_video.mp4\"  \n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Read the first frame and display it\n",
    "ret, frame = cap.read()\n",
    "if ret:\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define the output video writer\n",
    "out = cv2.VideoWriter('output_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Stop if video ends\n",
    "    \n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Draw detections on the frame\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box coordinates\n",
    "            conf = box.conf[0]  # Confidence score\n",
    "            cls = int(box.cls[0])  # Class index\n",
    "            label = f\"{model.names[cls]} {conf:.2f}\"\n",
    "            \n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    out.write(frame)  # Save frame to video\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import more libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.kalman import KalmanFilter # type: ignore\n",
    "from scipy.optimize import linear_sum_assignment # type: ignore\n",
    "from scipy.spatial.distance import cosine # type: ignore\n",
    "from skimage.feature import hog # type: ignore\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(frame, bbox):\n",
    "    \"\"\"\n",
    "    Extract appearance features from a detected person to help with tracking.\n",
    "    \n",
    "    Args:\n",
    "        frame: Input video frame\n",
    "        bbox: Bounding box coordinates [x1, y1, x2, y2]\n",
    "    \n",
    "    Returns:\n",
    "        Combined feature vector of color and HOG features\n",
    "    \n",
    "    Key Concepts:\n",
    "    - Color features: Uses HSV color histogram which is more robust to lighting changes\n",
    "    - HOG features: Captures shape/edge information that helps distinguish different people\n",
    "    - Feature combination: Merges both types for more robust tracking\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = map(int, bbox[:4])\n",
    "    person_image = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    # Standardize image size for consistent feature extraction\n",
    "    person_image = cv2.resize(person_image, (64, 128))\n",
    "    \n",
    "    # Color histogram in HSV space (more robust to lighting changes)\n",
    "    hsv_image = cv2.cvtColor(person_image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv_image], [0, 1], None, [8, 8], [0, 180, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    color_features = hist.flatten()\n",
    "    \n",
    "    # HOG features for shape/edge information\n",
    "    gray_image = cv2.cvtColor(person_image, cv2.COLOR_BGR2GRAY)\n",
    "    hog_features = hog(gray_image, orientations=9, pixels_per_cell=(8, 8), \n",
    "                      cells_per_block=(2, 2), visualize=False)\n",
    "    \n",
    "    return np.concatenate([color_features, hog_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between two bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        bbox1, bbox2: Bounding box coordinates [x1, y1, x2, y2]\n",
    "    \n",
    "    Returns:\n",
    "        IoU score between 0 and 1\n",
    "    \n",
    "    Note: IoU is a key metric for measuring spatial overlap between detections\n",
    "    and predicted track locations.\n",
    "    \"\"\"\n",
    "    # Calculate intersection coordinates\n",
    "    x1 = max(bbox1[0], bbox2[0])\n",
    "    y1 = max(bbox1[1], bbox2[1])\n",
    "    x2 = min(bbox1[2], bbox2[2])\n",
    "    y2 = min(bbox1[3], bbox2[3])\n",
    "\n",
    "    # Calculate areas\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
    "    area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "\n",
    "    return intersection / union if union > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(feature1, feature2, bbox1, bbox2, time_diff):\n",
    "    \"\"\"\n",
    "    Compute overall similarity between two detections using multiple cues.\n",
    "    \n",
    "    Args:\n",
    "        feature1, feature2: Appearance feature vectors\n",
    "        bbox1, bbox2: Bounding box coordinates\n",
    "        time_diff: Time difference between detections in frames\n",
    "    \n",
    "    Returns:\n",
    "        Weighted similarity score combining appearance, spatial, and temporal information\n",
    "    \n",
    "    Note: You can adjust the weights (0.7, 0.2, 0.1) to change the importance of each factor\n",
    "    \"\"\"\n",
    "    appearance_sim = 1 - cosine(feature1, feature2)  # Feature similarity\n",
    "    spatial_sim = iou(bbox1, bbox2)                  # Spatial overlap\n",
    "    temporal_weight = np.exp(-time_diff / 30)        # Temporal decay\n",
    "    \n",
    "    # Weighted combination - adjust these weights based on your needs\n",
    "    return appearance_sim * 0.7 + spatial_sim * 0.2 + temporal_weight * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_color(idx):\n",
    "    \"\"\"\n",
    "    Generate consistent colors for visualization based on track ID.\n",
    "    \n",
    "    Args:\n",
    "        idx: Track ID number\n",
    "    \n",
    "    Returns:\n",
    "        RGB color tuple\n",
    "    \n",
    "    Note: Same track ID will always get the same color\n",
    "    \"\"\"\n",
    "    np.random.seed(idx)\n",
    "    color = tuple(map(int, np.random.randint(0, 255, 3)))\n",
    "    return color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class represents the internal state of individual tracked objects using Kalman filter.\n",
    "\n",
    "Key Features:\n",
    "- Maintains position and velocity estimates\n",
    "- Tracks confidence and feature history\n",
    "- Handles track initialization and updates\n",
    "\n",
    "Important Parameters:\n",
    "- max_feature_history: Number of past features to store (default: 10)\n",
    "- reid_confidence: Track reliability score (0.0 to 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanBoxTracker:\n",
    "    count = 0  # Class variable for unique ID generation\n",
    "    \n",
    "    def __init__(self, bbox):\n",
    "        \"\"\"Initialize tracker with bounding box detection.\"\"\"\n",
    "        # Initialize Kalman filter with 7 state variables and 4 measurement variables\n",
    "        # State: [x, y, w, h, dx, dy, dw] (position, size, velocity)\n",
    "        # Measurement: [x, y, w, h] (position and size)\n",
    "        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
    "        \n",
    "        # Set up transition matrix (how state evolves)\n",
    "        self.kf.F = np.array([[1,0,0,0,1,0,0],  # x = x + dx\n",
    "                             [0,1,0,0,0,1,0],    # y = y + dy\n",
    "                             [0,0,1,0,0,0,1],    # w = w + dw\n",
    "                             [0,0,0,1,0,0,0],    # h = h\n",
    "                             [0,0,0,0,1,0,0],    # dx = dx\n",
    "                             [0,0,0,0,0,1,0],    # dy = dy\n",
    "                             [0,0,0,0,0,0,1]])   # dw = dw\n",
    "\n",
    "        # Measurement matrix (what we can measure)\n",
    "        self.kf.H = np.array([[1,0,0,0,0,0,0],  # We can measure x\n",
    "                             [0,1,0,0,0,0,0],    # We can measure y\n",
    "                             [0,0,1,0,0,0,0],    # We can measure w\n",
    "                             [0,0,0,1,0,0,0]])   # We can measure h\n",
    "\n",
    "        # Tune these parameters based on your needs:\n",
    "        self.kf.R[2:,2:] *= 10.    # Measurement noise\n",
    "        self.kf.P[4:,4:] *= 1000.  # Initial state uncertainty\n",
    "        self.kf.P *= 10.           # Initial state uncertainty\n",
    "        self.kf.Q[-1,-1] *= 0.01   # Process noise\n",
    "        self.kf.Q[4:,4:] *= 0.01   # Process noise\n",
    "\n",
    "        # Initialize state with first detection\n",
    "        self.kf.x[:4] = bbox[:4].reshape(4,1)\n",
    "        \n",
    "        # Track metadata\n",
    "        self.time_since_update = 0\n",
    "        self.id = self.get_next_id()\n",
    "        self.history = []\n",
    "        self.hits = 0                    # Number of detections\n",
    "        self.hit_streak = 0              # Consecutive detections\n",
    "        self.age = 0                     # Total frames\n",
    "        self.reid_confidence = 0.5       # Track confidence score\n",
    "        self.feature_history = []        # Store appearance features\n",
    "        self.max_feature_history = 10    # Maximum features to store\n",
    "        self.last_position = bbox[:4]    # Last known position\n",
    "        self.last_seen_frame = 0         # Frame number when last seen\n",
    "\n",
    "    @classmethod\n",
    "    def get_next_id(cls):\n",
    "        \"\"\"Generate unique track IDs.\"\"\"\n",
    "        id = cls.count\n",
    "        cls.count += 1\n",
    "        return id\n",
    "\n",
    "    def update_features(self, feature):\n",
    "        \"\"\"Update appearance feature history.\"\"\"\n",
    "        self.feature_history.append(feature)\n",
    "        if len(self.feature_history) > self.max_feature_history:\n",
    "            self.feature_history.pop(0)\n",
    "\n",
    "    def get_average_feature(self):\n",
    "        \"\"\"Get average appearance feature from history.\"\"\"\n",
    "        if not self.feature_history:\n",
    "            return None\n",
    "        return np.mean(self.feature_history, axis=0)\n",
    "\n",
    "    def update(self, bbox, frame_count=None):\n",
    "        \"\"\"\n",
    "        Update tracker state with new detection.\n",
    "        \n",
    "        Args:\n",
    "            bbox: New detection bounding box\n",
    "            frame_count: Current frame number (optional)\n",
    "        \"\"\"\n",
    "        self.time_since_update = 0\n",
    "        self.history = []\n",
    "        self.hits += 1\n",
    "        self.hit_streak += 1\n",
    "        self.kf.update(bbox[:4].reshape(4,1))\n",
    "        self.reid_confidence = min(1.0, self.reid_confidence + 0.1)\n",
    "        self.last_position = bbox[:4]\n",
    "        if frame_count is not None:\n",
    "            self.last_seen_frame = frame_count\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Advance tracker state and return predicted bounding box estimate.\n",
    "        \n",
    "        Returns:\n",
    "            Predicted bounding box location\n",
    "        \"\"\"\n",
    "        if((self.kf.x[6]+self.kf.x[2])<=0):\n",
    "            self.kf.x[6] *= 0.0\n",
    "        self.kf.predict()\n",
    "        self.age += 1\n",
    "        if(self.time_since_update>0):\n",
    "            self.hit_streak = 0\n",
    "        self.time_since_update += 1\n",
    "        self.history.append(self.kf.x)\n",
    "        self.reid_confidence = max(0.0, self.reid_confidence - 0.05)\n",
    "        return self.history[-1]\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Return current bounding box estimate.\"\"\"\n",
    "        return self.kf.x[:4].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OC-SORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OC-SORT implementation optimized for dynamic videos.\n",
    "\n",
    "Key Features:\n",
    "- Robust tracking during rapid movements\n",
    "- Identity recovery after occlusions\n",
    "- Appearance feature matching\n",
    "\n",
    "Customization Points:\n",
    "- Adjust weights between appearance, position, and motion\n",
    "- Tune thresholds for different movement patterns\n",
    "- Modify temporal windows for different video types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCSort:    \n",
    "    def __init__(self, \n",
    "                 det_thresh=0.3,        # Detection confidence threshold\n",
    "                 max_age=45,            # Maximum frames to keep dead tracks\n",
    "                 min_hits=2,            # Minimum hits to initialize track\n",
    "                 iou_threshold=0.2,     # IOU threshold for matching\n",
    "                 max_history=50,        # Maximum frames of history\n",
    "                 id_cooldown=90,        # Frames before ID can be reused\n",
    "                 delta_t=2,             # Time window for prediction\n",
    "                 appearance_weight=0.55, # Weight for appearance matching\n",
    "                 position_weight=0.15,   # Weight for spatial matching\n",
    "                 motion_weight=0.30):    # Weight for motion prediction\n",
    "        \"\"\"\n",
    "        Initialize tracker with customizable parameters.\n",
    "        \n",
    "        Tip: Adjust these parameters based on your video characteristics:\n",
    "        - For faster movements: decrease iou_threshold, increase motion_weight\n",
    "        - For more pose changes: decrease appearance_weight\n",
    "        - For longer occlusions: increase max_age, id_cooldown\n",
    "        \"\"\"\n",
    "        # Core tracking parameters\n",
    "        self.max_age = max_age\n",
    "        self.min_hits = min_hits\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.det_thresh = det_thresh\n",
    "        self.max_history = max_history\n",
    "        self.id_cooldown = id_cooldown\n",
    "        self.delta_t = delta_t\n",
    "        \n",
    "        # Matching weights\n",
    "        self.appearance_weight = appearance_weight\n",
    "        self.position_weight = position_weight\n",
    "        self.motion_weight = motion_weight\n",
    "        \n",
    "        # Track management\n",
    "        self.trackers = []\n",
    "        self.frame_count = 0\n",
    "        self.lost_trackers = []\n",
    "        self.recent_detections = []\n",
    "        self.inactive_trackers = {}\n",
    "        self.id_feature_history = {}\n",
    "        \n",
    "        # Exercise-specific parameters\n",
    "        self.temporal_window = 45       # Frames to look back for recovery\n",
    "        self.vertical_motion_threshold = 1.5  # For jumping movements\n",
    "        self.size_change_threshold = 0.4      # For pose changes\n",
    "        \n",
    "        # Recovery thresholds\n",
    "        self.recovery_similarity_threshold = 0.55\n",
    "        self.reid_threshold = 0.6\n",
    "\n",
    "        # Exercise-specific parameters\n",
    "        self.vertical_motion_threshold = 1.5  # Allow for jumping motion\n",
    "        self.size_change_threshold = 0.4      # Allow for pose changes\n",
    "\n",
    "    def update(self, dets, features=None):\n",
    "        self.frame_count += 1\n",
    "        \n",
    "        if len(dets) == 0:\n",
    "            return np.empty((0, 6))\n",
    "\n",
    "        # Clean up old inactive trackers\n",
    "        self._cleanup_inactive_trackers()\n",
    "\n",
    "        trks = np.zeros((len(self.trackers), 5))\n",
    "        to_del = []\n",
    "        for t, trk in enumerate(self.trackers):\n",
    "            pos = trk.predict()\n",
    "            if np.any(np.isnan(pos)):\n",
    "                to_del.append(t)\n",
    "            else:\n",
    "                if pos.ndim == 2 and pos.shape[1] == 1:\n",
    "                    pos = pos[:, 0]\n",
    "                if len(pos) >= 4:\n",
    "                    trks[t, :4] = pos[:4]\n",
    "                    trks[t, 4] = trk.time_since_update\n",
    "                else:\n",
    "                    print(f\"Warning: unexpected pos shape: {pos.shape}\")\n",
    "                    to_del.append(t)\n",
    "\n",
    "        trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n",
    "        for t in reversed(to_del):\n",
    "            self._deactivate_tracker(self.trackers[t])\n",
    "            self.trackers.pop(t)\n",
    "\n",
    "        matched, unmatched_dets, unmatched_trks = self.associate_detections_to_trackers(dets, trks, features)\n",
    "\n",
    "        # Update matched trackers with assigned detections\n",
    "        for m in matched:\n",
    "            self.trackers[m[1]].update(dets[m[0], :], self.frame_count)\n",
    "            if features is not None and m[0] < len(features):\n",
    "                self.trackers[m[1]].update_features(features[m[0]])\n",
    "                self._update_id_features(self.trackers[m[1]].id, features[m[0]])\n",
    "\n",
    "        # Create and initialise new trackers for unmatched detections\n",
    "        for i in unmatched_dets:\n",
    "            trk = KalmanBoxTracker(dets[i,:])\n",
    "            if features is not None and i < len(features):\n",
    "                trk.update_features(features[i])\n",
    "                self._update_id_features(trk.id, features[i])\n",
    "            self.trackers.append(trk)\n",
    "\n",
    "        # Store current detections\n",
    "        if len(dets) > 0:\n",
    "            self.recent_detections.append({\n",
    "                'frame': self.frame_count,\n",
    "                'detections': dets.copy(),\n",
    "                'features': features.copy() if features is not None else None\n",
    "            })\n",
    "            if len(self.recent_detections) > self.temporal_window:\n",
    "                self.recent_detections.pop(0)\n",
    "\n",
    "        # Update tracker states and handle lost trackers\n",
    "        ret = []\n",
    "        for i, trk in reversed(list(enumerate(self.trackers))):\n",
    "            d = trk.get_state()\n",
    "            if trk.time_since_update > self.max_age:\n",
    "                self._deactivate_tracker(trk)\n",
    "                self.trackers.pop(i)\n",
    "            elif trk.hits >= self.min_hits or self.frame_count <= self.min_hits:\n",
    "                ret.append(np.concatenate((d, [trk.id+1], [trk.reid_confidence])).reshape(1, -1))\n",
    "\n",
    "        # Try to recover lost trackers that are still within cooldown period\n",
    "        self._attempt_track_recovery()\n",
    "\n",
    "        if len(ret) > 0:\n",
    "            return np.concatenate(ret)\n",
    "        return np.empty((0, 6))\n",
    "\n",
    "    def _deactivate_tracker(self, tracker):\n",
    "        \"\"\"Store tracker information when it becomes inactive\"\"\"\n",
    "        self.inactive_trackers[tracker.id] = {\n",
    "            'last_frame': self.frame_count,\n",
    "            'last_position': tracker.last_position,\n",
    "            'features': tracker.feature_history.copy() if tracker.feature_history else None,\n",
    "            'reid_confidence': tracker.reid_confidence\n",
    "        }\n",
    "\n",
    "    def _cleanup_inactive_trackers(self):\n",
    "        \"\"\"Remove inactive trackers that have exceeded the cooldown period\"\"\"\n",
    "        current_frame = self.frame_count\n",
    "        ids_to_remove = []\n",
    "        \n",
    "        for track_id, info in self.inactive_trackers.items():\n",
    "            if current_frame - info['last_frame'] > self.id_cooldown:\n",
    "                ids_to_remove.append(track_id)\n",
    "                if track_id in self.id_feature_history:\n",
    "                    del self.id_feature_history[track_id]\n",
    "        \n",
    "        for track_id in ids_to_remove:\n",
    "            del self.inactive_trackers[track_id]\n",
    "\n",
    "    def _update_id_features(self, track_id, feature):\n",
    "        \"\"\"Update the feature history for a track ID\"\"\"\n",
    "        if track_id not in self.id_feature_history:\n",
    "            self.id_feature_history[track_id] = []\n",
    "        \n",
    "        self.id_feature_history[track_id].append(feature)\n",
    "        if len(self.id_feature_history[track_id]) > self.max_history:\n",
    "            self.id_feature_history[track_id].pop(0)\n",
    "\n",
    "    def _attempt_track_recovery(self):\n",
    "        \"\"\"Enhanced track recovery with motion prediction and appearance matching\"\"\"\n",
    "        if not self.recent_detections:\n",
    "            return\n",
    "\n",
    "        latest_detections = self.recent_detections[-1]\n",
    "        current_frame = self.frame_count\n",
    "        \n",
    "        recovered_ids = []\n",
    "        for track_id, info in self.inactive_trackers.items():\n",
    "            if current_frame - info['last_frame'] <= self.id_cooldown:\n",
    "                detections = latest_detections['detections']\n",
    "                features = latest_detections['features']\n",
    "                \n",
    "                if features is None or len(features) == 0:\n",
    "                    continue\n",
    "\n",
    "                best_match = None\n",
    "                best_similarity = -1\n",
    "                \n",
    "                # Predict position based on last known velocity\n",
    "                predicted_position = info['last_position']\n",
    "                if 'velocity' in info:\n",
    "                    time_diff = current_frame - info['last_frame']\n",
    "                    predicted_position = self._predict_position(info['last_position'], \n",
    "                                                             info['velocity'], \n",
    "                                                             time_diff)\n",
    "\n",
    "                for i, (det, feat) in enumerate(zip(detections, features)):\n",
    "                    # Calculate multiple similarity metrics\n",
    "                    position_similarity = iou(predicted_position, det)\n",
    "                    feature_similarity = 1 - cosine(np.mean(info['features'], axis=0), feat)\n",
    "                    \n",
    "                    # Calculate motion consistency\n",
    "                    motion_similarity = 1.0\n",
    "                    if 'velocity' in info:\n",
    "                        current_velocity = (det[:2] - info['last_position'][:2]) / (current_frame - info['last_frame'])\n",
    "                        motion_similarity = np.exp(-np.linalg.norm(current_velocity - info['velocity']) / 2.0)\n",
    "\n",
    "                    # Weighted combination of similarities\n",
    "                    total_similarity = (\n",
    "                        self.appearance_weight * feature_similarity +\n",
    "                        self.position_weight * position_similarity +\n",
    "                        self.motion_weight * motion_similarity\n",
    "                    )\n",
    "\n",
    "                    if total_similarity > best_similarity and total_similarity > self.recovery_similarity_threshold:\n",
    "                        best_similarity = total_similarity\n",
    "                        best_match = i\n",
    "\n",
    "                if best_match is not None:\n",
    "                    new_tracker = KalmanBoxTracker(detections[best_match])\n",
    "                    new_tracker.id = track_id\n",
    "                    new_tracker.feature_history = info['features']\n",
    "                    new_tracker.reid_confidence = min(info['reid_confidence'] + 0.1, 1.0)\n",
    "                    new_tracker.update(detections[best_match], self.frame_count)\n",
    "                    new_tracker.update_features(features[best_match])\n",
    "                    \n",
    "                    # Update velocity information\n",
    "                    if 'last_position' in info:\n",
    "                        time_diff = current_frame - info['last_frame']\n",
    "                        if time_diff > 0:\n",
    "                            velocity = (detections[best_match][:2] - info['last_position'][:2]) / time_diff\n",
    "                            new_tracker.velocity = velocity\n",
    "                    \n",
    "                    self.trackers.append(new_tracker)\n",
    "                    recovered_ids.append(track_id)\n",
    "\n",
    "        # Remove recovered trackers from inactive list\n",
    "        for track_id in recovered_ids:\n",
    "            del self.inactive_trackers[track_id]\n",
    "\n",
    "    def _predict_position(self, last_pos, velocity, time_diff):\n",
    "        \"\"\"\n",
    "        Enhanced position prediction considering exercise movements\n",
    "        \"\"\"\n",
    "        predicted = last_pos.copy()\n",
    "        \n",
    "        # Predict x, y position\n",
    "        predicted[:2] += velocity[:2] * time_diff\n",
    "        \n",
    "        # Predict box size with damped changes for jumping motions\n",
    "        size_velocity = velocity[2:]  # velocity of width and height changes\n",
    "        predicted[2:] += size_velocity * time_diff * 0.7  # dampen size changes\n",
    "        \n",
    "        return predicted\n",
    "\n",
    "    def _calculate_box_motion_state(self, det1, det2, frame_diff):\n",
    "        \"\"\"\n",
    "        Calculate motion state including vertical movement and size changes\n",
    "        \"\"\"\n",
    "        # Extract box coordinates\n",
    "        x1, y1, x2, y2 = det1[:4]\n",
    "        prev_x1, prev_y1, prev_x2, prev_y2 = det2[:4]\n",
    "        \n",
    "        # Calculate center points\n",
    "        center_y = (y1 + y2) / 2\n",
    "        prev_center_y = (prev_y1 + prev_y2) / 2\n",
    "        \n",
    "        # Calculate box sizes\n",
    "        current_height = y2 - y1\n",
    "        prev_height = prev_y2 - prev_y1\n",
    "        current_width = x2 - x1\n",
    "        prev_width = prev_x2 - prev_x1\n",
    "        \n",
    "        # Calculate motion metrics\n",
    "        vertical_speed = abs(center_y - prev_center_y) / frame_diff\n",
    "        height_change = abs(current_height - prev_height) / max(prev_height, 1e-6)\n",
    "        width_change = abs(current_width - prev_width) / max(prev_width, 1e-6)\n",
    "        \n",
    "        return {\n",
    "            'vertical_speed': vertical_speed,\n",
    "            'height_change': height_change,\n",
    "            'width_change': width_change\n",
    "        }\n",
    "\n",
    "    def associate_detections_to_trackers(self, detections, trackers, features=None):\n",
    "        if len(trackers) == 0:\n",
    "            return np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0,), dtype=int)\n",
    "\n",
    "        iou_matrix = np.zeros((len(detections), len(trackers)))\n",
    "        feature_matrix = np.zeros((len(detections), len(trackers)))\n",
    "        motion_matrix = np.zeros((len(detections), len(trackers)))\n",
    "        \n",
    "        # Calculate standard IoU matrix\n",
    "        for d, det in enumerate(detections):\n",
    "            for t, trk in enumerate(trackers):\n",
    "                iou_matrix[d, t] = iou(det, trk)\n",
    "\n",
    "        # Enhanced feature matching considering pose variations\n",
    "        if features is not None and len(self.trackers) > 0:\n",
    "            for i, detection_feature in enumerate(features):\n",
    "                for j, tracker in enumerate(self.trackers):\n",
    "                    if tracker.get_average_feature() is not None:\n",
    "                        # Calculate feature similarity with temporal weighting\n",
    "                        recent_features = tracker.feature_history[-3:]  # Consider last 3 features\n",
    "                        if recent_features:\n",
    "                            similarities = [1 - cosine(detection_feature, feat) \n",
    "                                         for feat in recent_features]\n",
    "                            # Give more weight to recent features\n",
    "                            weights = np.exp(np.linspace(-1, 0, len(similarities)))\n",
    "                            feature_matrix[i, j] = np.average(similarities, weights=weights)\n",
    "\n",
    "        # Enhanced motion matching for exercise movements\n",
    "        for d, det in enumerate(detections):\n",
    "            for t, trk in enumerate(self.trackers):\n",
    "                if hasattr(trk, 'velocity'):\n",
    "                    predicted_pos = self._predict_position(trk.last_position, \n",
    "                                                        trk.velocity, \n",
    "                                                        1)\n",
    "                    \n",
    "                    # Calculate motion state if we have previous detection\n",
    "                    if hasattr(trk, 'last_position'):\n",
    "                        motion_state = self._calculate_box_motion_state(\n",
    "                            det, trk.last_position, 1)\n",
    "                        \n",
    "                        # Adjust motion similarity based on exercise patterns\n",
    "                        motion_similarity = np.exp(-np.linalg.norm(det[:2] - predicted_pos[:2]) / 50.0)\n",
    "                        \n",
    "                        # Be more lenient with vertical motion during jumping\n",
    "                        if motion_state['vertical_speed'] > self.vertical_motion_threshold:\n",
    "                            motion_similarity *= 1.2\n",
    "                        \n",
    "                        # Be more lenient with size changes during pose changes\n",
    "                        if motion_state['height_change'] > self.size_change_threshold:\n",
    "                            motion_similarity *= 1.1\n",
    "                            \n",
    "                        motion_matrix[d, t] = motion_similarity\n",
    "\n",
    "        # Combine matrices with exercise-optimized weights\n",
    "        similarity_matrix = (\n",
    "            self.appearance_weight * feature_matrix +\n",
    "            self.position_weight * iou_matrix +\n",
    "            self.motion_weight * motion_matrix\n",
    "        )\n",
    "\n",
    "        # Perform matching\n",
    "        matched_indices = linear_sum_assignment(-similarity_matrix)\n",
    "        matched_indices = np.asarray(matched_indices)\n",
    "        matched_indices = np.transpose(matched_indices)\n",
    "\n",
    "        unmatched_detections = []\n",
    "        for d in range(len(detections)):\n",
    "            if d not in matched_indices[:, 0]:\n",
    "                unmatched_detections.append(d)\n",
    "\n",
    "        unmatched_trackers = []\n",
    "        for t in range(len(trackers)):\n",
    "            if t not in matched_indices[:, 1]:\n",
    "                unmatched_trackers.append(t)\n",
    "\n",
    "        # Filter matches with adaptive threshold\n",
    "        matches = []\n",
    "        for m in matched_indices:\n",
    "            match_similarity = similarity_matrix[m[0], m[1]]\n",
    "            \n",
    "            # Calculate adaptive threshold based on motion\n",
    "            threshold = self.iou_threshold\n",
    "            if hasattr(self.trackers[m[1]], 'last_position'):\n",
    "                motion_state = self._calculate_box_motion_state(\n",
    "                    detections[m[0]], \n",
    "                    self.trackers[m[1]].last_position,\n",
    "                    1\n",
    "                )\n",
    "                # Lower threshold during high motion periods\n",
    "                if motion_state['vertical_speed'] > self.vertical_motion_threshold:\n",
    "                    threshold *= 0.8\n",
    "                if motion_state['height_change'] > self.size_change_threshold:\n",
    "                    threshold *= 0.8\n",
    "            \n",
    "            if match_similarity < threshold:\n",
    "                unmatched_detections.append(m[0])\n",
    "                unmatched_trackers.append(m[1])\n",
    "            else:\n",
    "                matches.append(m.reshape(1, 2))\n",
    "\n",
    "        if len(matches) == 0:\n",
    "            matches = np.empty((0, 2), dtype=int)\n",
    "        else:\n",
    "            matches = np.concatenate(matches, axis=0)\n",
    "\n",
    "        return matches, np.array(unmatched_detections), np.array(unmatched_trackers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OC-SORT Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = OCSort(\n",
    "        det_thresh=0.3,          # Detection confidence threshold\n",
    "        max_age=60,              # Maximum frames to keep dead tracks\n",
    "        min_hits=3,              # Minimum detections to initialize track\n",
    "        iou_threshold=0.2,       # IOU threshold for matching\n",
    "        max_history=40,          # Maximum frames of history to keep\n",
    "        id_cooldown=120,         # Frames before ID can be reused\n",
    "        delta_t=2,               # Time window for prediction\n",
    "        \n",
    "        # Matching weights - adjust these based on your needs:\n",
    "        appearance_weight=0.6,   # Weight for appearance matching\n",
    "        position_weight=0.2,     # Weight for spatial matching\n",
    "        motion_weight=0.3        # Weight for motion prediction\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Video for Detection & Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'yolo_demo_video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"Video properties: {width}x{height} @ {fps}fps, {total_frames} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Video Frame-by-Frame with YOLO and OC-SORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 0\n",
    "processing_times = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # Original FPS\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Define codec and create VideoWriter object to save output video\n",
    "output_path = \"ocsort_output_video.mp4\"  # Specify output path\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4 file\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Main processing loop\n",
    "while True:\n",
    "    # Read frame from video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame_count += 1\n",
    "    frame_start_time = time.time()\n",
    "    \n",
    "    # Step 1: Object Detection\n",
    "    # Run YOLOv8 detection on the frame\n",
    "    results = model(frame)\n",
    "\n",
    "    # Step 2: Process Detections\n",
    "    # Extract person detections and compute features\n",
    "    detections = []\n",
    "    features = []\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            # Modify here for class-specific tracking  \n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            conf = box.conf[0]\n",
    "            # Filter by confidence threshold\n",
    "            if conf >= 0.8:  # Adjust this threshold based on your needs\n",
    "                detection = [x1, y1, x2, y2, conf]\n",
    "                detections.append(detection)\n",
    "                # Compute appearance features for each detection\n",
    "                features.append(extract_features(frame, detection))\n",
    "\n",
    "    # Step 3: Update Tracking\n",
    "    if len(detections) > 0:\n",
    "        # Convert lists to numpy arrays for tracking\n",
    "        detections = np.array(detections)\n",
    "        features = np.array(features)\n",
    "\n",
    "        # Update tracker with new detections\n",
    "        tracked_objects = tracker.update(detections, features)\n",
    "\n",
    "        # Step 4: Visualization\n",
    "        for obj in tracked_objects:\n",
    "            x1, y1, x2, y2, track_id, reid_conf = obj\n",
    "            # Generate consistent color for this track ID\n",
    "            color = generate_color(int(track_id))\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "            \n",
    "            # Add text background for better visibility\n",
    "            text = f\"ID: {int(track_id)}\"\n",
    "            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.9, 2)\n",
    "            cv2.rectangle(frame, (int(x1), int(y1) - text_height - 10), \n",
    "                        (int(x1) + text_width, int(y1)), color, -1)\n",
    "            \n",
    "            # Add track ID text\n",
    "            cv2.putText(frame, text, (int(x1), int(y1) - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "\n",
    "    # Step 5: Performance Monitoring\n",
    "    # Calculate and track processing speed\n",
    "    processing_time = time.time() - frame_start_time\n",
    "    processing_times.append(processing_time)\n",
    "    \n",
    "    # Calculate progress and estimated time remaining\n",
    "    elapsed_time = time.time() - start_time\n",
    "    progress = frame_count / total_frames\n",
    "    if progress > 0:\n",
    "        eta = elapsed_time * (1 - progress) / progress\n",
    "        eta_str = time.strftime('%H:%M:%S', time.gmtime(eta))\n",
    "    else:\n",
    "        eta_str = \"Calculating...\"\n",
    "\n",
    "    # Calculate average processing speed\n",
    "    avg_time = sum(processing_times[-30:]) / min(len(processing_times), 30)\n",
    "    avg_fps = 1.0 / avg_time if avg_time > 0 else 0\n",
    "    \n",
    "    # Prepare information text for display\n",
    "    info_text = [\n",
    "        f\"Frame: {frame_count}/{total_frames}\",\n",
    "        f\"Progress: {progress*100:.1f}%\",\n",
    "        f\"ETA: {eta_str}\",\n",
    "        f\"FPS: {avg_fps:.1f}\"\n",
    "    ]\n",
    "    \n",
    "    # Add processing information to frame\n",
    "    y_position = 30\n",
    "    for text in info_text:\n",
    "        # Add background rectangle for better text visibility\n",
    "        (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        cv2.rectangle(frame, (10, y_position - text_height - 5), \n",
    "                    (10 + text_width, y_position + 5), (0, 0, 0), -1)\n",
    "        # Add text\n",
    "        cv2.putText(frame, text, (10, y_position),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        y_position += 30\n",
    "\n",
    "    # Write frame to output video\n",
    "    out.write(frame)\n",
    "    \n",
    "    # Print progress to console (update every 30 frames)\n",
    "    if frame_count % 30 == 0:\n",
    "        print(f\"\\rProcessing: {progress*100:.1f}% | FPS: {avg_fps:.1f} | ETA: {eta_str}\", end=\"\")\n",
    "    \n",
    "    # Check for 'q' key press to exit early\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Step 6: Final Statistics\n",
    "total_time = time.time() - start_time\n",
    "avg_processing_time = sum(processing_times) / len(processing_times)\n",
    "\n",
    "# Print processing summary\n",
    "print(f\"\\n\\nProcessing complete!\")\n",
    "print(f\"Output saved to: {output_path}\")\n",
    "print(f\"Total processing time: {total_time:.1f} seconds\")\n",
    "print(f\"Average processing time per frame: {avg_processing_time:.3f} seconds\")\n",
    "print(f\"Average processing FPS: {1.0/avg_processing_time:.1f}\")\n",
    "print(f\"Input video FPS: {fps}\")\n",
    "print(f\"Total frames processed: {frame_count}\")\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "* https://docs.ultralytics.com/models/yolov8/\n",
    "* https://www.freecodecamp.org/news/how-to-detect-objects-in-images-using-yolov8/\n",
    "* https://medium.com/@itberrios6/introduction-to-ocsort-c1ea1c6adfa2\n",
    "* https://github.com/itberrios/CV_tracking/blob/main/setup_tutorials/tutorial_yolo_nas_and_ocsort.ipynb\n",
    "* https://github.com/noahcao/OC_SORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Student Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the YOLO + OC-SORT knowledge to solve a challenge using the starter code provided.\n",
    "\n",
    "- **Challenge Options (Choose One)**:\n",
    "    - **Filter by Object Class:** Track only a specific object (e.g., people or cars).\n",
    "    - **Display Object Tracking History:** Show a history of tracked objects with movement paths.\n",
    "    - **Track Objects by Color:** Track objects based on a specific color (e.g., red).\n",
    "    - **Add Object Confidence Threshold:** Ignore low-confidence detections and track only high-confidence objects.\n",
    "    - **Display Object Count:** Count and display the number of detected objects of each class.\n",
    "    - **Object Speed Estimation:** Estimate and display the speed of each tracked object.\n",
    "    - **Multi-Object Interaction Highlight:** Change the color when two objects are in close proximity.\n",
    "\n",
    "- **Instructions:**\n",
    "   - Use the starter code and modify it to implement the chosen challenge.\n",
    "   - Test and run the solution, then demonstrate the results.\n",
    "   - Discuss any challenges faced and how they were overcome.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
